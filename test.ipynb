{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import click\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n",
    "\n",
    "\n",
    "def save_params_with_name(params, name):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('{}.p'.format(name), 'wb'))\n",
    "\n",
    "\n",
    "def load_params_with_name(name):\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('{}.p'.format(name), mode='rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = max(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 13 integer features and 26 categorical features\n",
    "#创建列表\n",
    "continous_features = range(1, 14)\n",
    "categorial_features = range(14, 40)\n",
    "\n",
    "# Clip integer features. The clip point for each integer feature\n",
    "# is derived from the 95% quantile of the total values in each feature\n",
    "#使列数据平滑，去掉异常大的值\n",
    "continous_clip = [20, 600, 100, 50, 64000, 500, 100, 50, 500, 10, 10, 10, 50]\n",
    "\n",
    "#连续特征生成器\n",
    "class ContinuousFeatureGenerator:\n",
    "    \"\"\"\n",
    "    Normalize the integer features to [0, 1] by min-max normalization\n",
    "    将数值型特征归一化到[0,1]\n",
    "    \"\"\"\n",
    "    #num_feature为数值型特征的种类数\n",
    "    def __init__(self, num_feature):\n",
    "        self.num_feature = num_feature\n",
    "        #生成长度为num_feature的列表\n",
    "        self.min1 = [sys.maxsize] * num_feature\n",
    "        self.max1 = [-sys.maxsize] * num_feature\n",
    "\n",
    "    def build(self, datafile, continous_features):\n",
    "#         data = pd.read_csv(datafile)\n",
    "#         for line \n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                #print(line)\n",
    "                #生成每行特征值的列表，长度为40\n",
    "                features = line.split(',')\n",
    "                #print(features)\n",
    "                #对数值型特征求最大值和最小值，分行求，存储在min1和max1中\n",
    "                for i in range(0, self.num_feature):\n",
    "                    #print(len(features))\n",
    "                    val = features[continous_features[i]]\n",
    "                    #print(val) \n",
    "                    if val != '':\n",
    "                        val = int(val)\n",
    "                        #如果数据过大，则进行截断\n",
    "                        if val > continous_clip[i]:\n",
    "                            val = continous_clip[i]\n",
    "                        #print(val)\n",
    "                        #a=self.min[i]\n",
    "                        #b=self.max[i]\n",
    "                        #对每列特征的最小值进行对比替换\n",
    "                        \n",
    "                        self.min1[i] = min(self.min1[i], val)\n",
    "                        self.max1[i] = max(self.max1[i], val)\n",
    "\n",
    "    #对数值型特征值进行[0,1]归一化\n",
    "    def gen(self, idx, val):\n",
    "        if val == '':\n",
    "            return 0.0\n",
    "        val = float(val)\n",
    "        return (val - self.min1[idx]) / (self.max1[idx] - self.min1[idx])\n",
    "\n",
    "class CategoryDictGenerator:\n",
    "    \"\"\"\n",
    "    Generate dictionary for each of the categorical features\n",
    "    出现次数大于cutoff的类别留下\n",
    "    \"\"\"\n",
    "    #类别数目num_feature为26\n",
    "    def __init__(self, num_feature):\n",
    "        self.dicts = []\n",
    "        self.num_feature = num_feature\n",
    "        #产生元素为字典的列表，其长度为26\n",
    "        for i in range(0, num_feature):\n",
    "            self.dicts.append(collections.defaultdict(int))\n",
    "\n",
    "    def build(self, datafile, categorial_features, cutoff=0):\n",
    "        with open(datafile, 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.split(',')\n",
    "                #计算每一列不同特征出现的次数\n",
    "                for i in range(0, self.num_feature):\n",
    "                    if features[categorial_features[i]] != '':\n",
    "                        self.dicts[i][features[categorial_features[i]]] += 1\n",
    "            #print(self.dicts)\n",
    "        #过滤掉出现次数小于cutoff的特征,将其key值设为unk，并进行排序\n",
    "        for i in range(0, self.num_feature):\n",
    "            self.dicts[i] = filter(lambda x: x[1] >= cutoff,\n",
    "                                   self.dicts[i].items())\n",
    "\n",
    "            self.dicts[i] = sorted(self.dicts[i], key=lambda x: (-x[1], x[0]))\n",
    "            vocabs, _ = list(zip(*self.dicts[i]))\n",
    "            self.dicts[i] = dict(zip(vocabs, range(1, len(vocabs) + 1)))\n",
    "            self.dicts[i]['<unk>'] = 0\n",
    "\n",
    "    def gen(self, idx, key):\n",
    "        if key not in self.dicts[idx]:\n",
    "            res = self.dicts[idx]['<unk>']\n",
    "        else:\n",
    "            res = self.dicts[idx][key]\n",
    "        return res\n",
    "    #计算每一列过滤后的特征长度，生成长度列表\n",
    "    def dicts_sizes(self):\n",
    "        return list(map(len, self.dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {})\n"
     ]
    }
   ],
   "source": [
    "a=collections.defaultdict(int)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datadir, outdir):\n",
    "    \"\"\"\n",
    "    All the 13 integer features are normalzied to continous values and these\n",
    "    continous features are combined into one vecotr with dimension 13.\n",
    "\n",
    "    Each of the 26 categorical features are one-hot encoded and all the one-hot\n",
    "    vectors are combined into one sparse binary vector.\n",
    "    \"\"\"\n",
    "    #计算生成每一列数值型特征的最大值和最小值\n",
    "    dists = ContinuousFeatureGenerator(len(continous_features))\n",
    "    dists.build(os.path.join(datadir, 'Train_400000.csv'), continous_features)\n",
    "    \n",
    "    #生成每一列类别型特征过滤后的特征字典\n",
    "    dicts = CategoryDictGenerator(len(categorial_features))\n",
    "    dicts.build(\n",
    "        os.path.join(datadir, 'Train_400000.csv'), categorial_features, cutoff=200)#200 50\n",
    "    \n",
    "    #返回每一列类别特征长度列表\n",
    "    dict_sizes = dicts.dicts_sizes()\n",
    "    categorial_feature_offset = [0]\n",
    "    for i in range(1, len(categorial_features)):\n",
    "        offset = categorial_feature_offset[i - 1] + dict_sizes[i - 1]\n",
    "        categorial_feature_offset.append(offset)\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    # 90% of the data are used for training, and 10% of the data are used\n",
    "    # for validation.\n",
    "    train_ffm = open(os.path.join(outdir, 'train_ffm.txt'), 'w')\n",
    "    valid_ffm = open(os.path.join(outdir, 'valid_ffm.txt'), 'w')\n",
    "\n",
    "    train_lgb = open(os.path.join(outdir, 'train_lgb.txt'), 'w')\n",
    "    valid_lgb = open(os.path.join(outdir, 'valid_lgb.txt'), 'w')\n",
    "\n",
    "    with open(os.path.join(outdir, 'train.txt'), 'w') as out_train:\n",
    "        with open(os.path.join(outdir, 'valid.txt'), 'w') as out_valid:\n",
    "            with open(os.path.join(datadir, 'Train_400000.csv'), 'r') as f:\n",
    "                for line in f:\n",
    "                    features = line.split(',')\n",
    "                    continous_feats = []\n",
    "                    continous_vals = []\n",
    "                    for i in range(0, len(continous_features)):\n",
    "                        val = dists.gen(i, features[continous_features[i]])\n",
    "                        continous_vals.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                        continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                    categorial_vals = []\n",
    "                    categorial_lgb_vals = []\n",
    "                    for i in range(0, len(categorial_features)):\n",
    "                        val = dicts.gen(i, features[categorial_features[i]]) + categorial_feature_offset[i]\n",
    "                        categorial_vals.append(str(val))\n",
    "                        val_lgb = dicts.gen(i, features[categorial_features[i]])\n",
    "                        categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                    continous_vals = ','.join(continous_vals)\n",
    "                    categorial_vals = ','.join(categorial_vals)\n",
    "                    label = features[0]\n",
    "                    if random.randint(0, 9999) % 10 != 0:\n",
    "                        out_train.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                        train_ffm.write('\\t'.join(label) + '\\t')\n",
    "                        train_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "                        train_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                        \n",
    "                        train_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        train_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "                    else:\n",
    "                        out_valid.write(','.join(\n",
    "                            [continous_vals, categorial_vals, label]) + '\\n')\n",
    "                        valid_ffm.write('\\t'.join(label) + '\\t')\n",
    "                        valid_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "                        valid_ffm.write('\\t'.join(\n",
    "                            ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                                                \n",
    "                        valid_lgb.write('\\t'.join(label) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                        valid_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "                        \n",
    "    train_ffm.close()\n",
    "    valid_ffm.close()\n",
    "\n",
    "    train_lgb.close()\n",
    "    valid_lgb.close()\n",
    "\n",
    "    test_ffm = open(os.path.join(outdir, 'test_ffm.txt'), 'w')\n",
    "    test_lgb = open(os.path.join(outdir, 'test_lgb.txt'), 'w')\n",
    "\n",
    "    with open(os.path.join(outdir, 'test.txt'), 'w') as out:\n",
    "        with open(os.path.join(datadir, 'Test_200000data.csv'), 'r') as f:\n",
    "            for line in f:\n",
    "                features = line.split(',')\n",
    "\n",
    "                continous_feats = []\n",
    "                continous_vals = []\n",
    "                for i in range(0, len(continous_features)):\n",
    "                    val = dists.gen(i, features[continous_features[i] - 1])\n",
    "                    continous_vals.append(\n",
    "                        \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))\n",
    "                    continous_feats.append(\n",
    "                            \"{0:.6f}\".format(val).rstrip('0').rstrip('.'))#('{0}'.format(val))\n",
    "\n",
    "                categorial_vals = []\n",
    "                categorial_lgb_vals = []\n",
    "                for i in range(0, len(categorial_features)):\n",
    "                    val = dicts.gen(i,\n",
    "                                    features[categorial_features[i] -\n",
    "                                             1]) + categorial_feature_offset[i]\n",
    "                    categorial_vals.append(str(val))\n",
    "\n",
    "                    val_lgb = dicts.gen(i, features[categorial_features[i] - 1])\n",
    "                    categorial_lgb_vals.append(str(val_lgb))\n",
    "\n",
    "                continous_vals = ','.join(continous_vals)\n",
    "                categorial_vals = ','.join(categorial_vals)\n",
    "\n",
    "                out.write(','.join([continous_vals, categorial_vals]) + '\\n')\n",
    "                \n",
    "                test_ffm.write('\\t'.join(['{}:{}:{}'.format(ii, ii, val) for ii,val in enumerate(continous_vals.split(','))]) + '\\t')\n",
    "                test_ffm.write('\\t'.join(\n",
    "                    ['{}:{}:1'.format(ii + 13, str(np.int32(val) + 13)) for ii, val in enumerate(categorial_vals.split(','))]) + '\\n')\n",
    "                                                                \n",
    "                test_lgb.write('\\t'.join(continous_feats) + '\\t')\n",
    "                test_lgb.write('\\t'.join(categorial_lgb_vals) + '\\n')\n",
    "\n",
    "    test_ffm.close()\n",
    "    test_lgb.close()\n",
    "    return dict_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sizes = preprocess('./data','./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params_with_name((dict_sizes), 'dict_sizes') #pickle.dump((dict_sizes), open('dict_sizes.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sizes = load_params_with_name('dict_sizes') #pickle.load(open('dict_sizes.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3805"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(dict_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# 训练FFM\n",
    "数据准备好了，开始调用LibFFM，训练FFM模型。\n",
    "\n",
    "learning rate是0.1，迭代32次，训练好后保存的模型文件是model_ffm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, sys, os, time\n",
    "\n",
    "NR_THREAD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libffm/libffm/ffm-train --auto-stop -r 0.1 -t 32 -s {nr_thread} -p ./data/valid_ffm.txt ./data/train_ffm.txt model_ffm'.format(nr_thread=NR_THREAD) \n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFM模型训练好了，我们把训练、验证和测试数据输入给FFM，得到FFM层的输出，输出的文件名为*.out.logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libffm/libffm/ffm-predict ./data/train_ffm.txt model_ffm tr_ffm.out'.format(nr_thread=NR_THREAD) \n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libffm/libffm/ffm-predict ./data/valid_ffm.txt model_ffm va_ffm.out'.format(nr_thread=NR_THREAD) \n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libffm/libffm/ffm-predict ./data/test_ffm.txt model_ffm te_ffm.out true'.format(nr_thread=NR_THREAD) \n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练GBDT¶\n",
    "现在调用LightGBM训练GBDT模型，因为决策树较容易过拟合，我们设置树的个数为32，叶子节点数设为30，深度就不设置了，学习率设为0.05。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_pred(tr_path, va_path, _sep = '\\t', iter_num = 32):\n",
    "    # load or create your dataset\n",
    "    print('Load data...')\n",
    "    df_train = pd.read_csv(tr_path, header=None, sep=_sep)\n",
    "    df_test = pd.read_csv(va_path, header=None, sep=_sep)\n",
    "    \n",
    "    y_train = df_train[0].values\n",
    "    y_test = df_test[0].values\n",
    "    X_train = df_train.drop(0, axis=1).values\n",
    "    X_test = df_test.drop(0, axis=1).values\n",
    "    \n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "    \n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': {'l2', 'auc', 'logloss'},\n",
    "        'num_leaves': 30,\n",
    "#         'max_depth': 7,\n",
    "        'num_trees': 32,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    print('Start training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=iter_num,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    feature_name=[\"I1\",\"I2\",\"I3\",\"I4\",\"I5\",\"I6\",\"I7\",\"I8\",\"I9\",\"I10\",\"I11\",\"I12\",\"I13\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"],\n",
    "                    categorical_feature=[\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\",\"C12\",\"C13\",\"C14\",\"C15\",\"C16\",\"C17\",\"C18\",\"C19\",\"C20\",\"C21\",\"C22\",\"C23\",\"C24\",\"C25\",\"C26\"],\n",
    "                    early_stopping_rounds=5)\n",
    "    \n",
    "    print('Save model...')\n",
    "    # save model to file\n",
    "    gbm.save_model('lgb_model.txt')\n",
    "    \n",
    "    print('Start predicting...')\n",
    "    # predict\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    # eval\n",
    "    print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
    "\n",
    "    return gbm,y_pred,X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wpp\\AppData\\Roaming\\Python\\Python36\\site-packages\\lightgbm\\engine.py:118: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\wpp\\AppData\\Roaming\\Python\\Python36\\site-packages\\lightgbm\\basic.py:1209: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['C1', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C2', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "C:\\Users\\wpp\\AppData\\Roaming\\Python\\Python36\\site-packages\\lightgbm\\basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's l2: 0.185497\tvalid_0's auc: 0.701065\n",
      "Training until validation scores don't improve for 5 rounds.\n",
      "[2]\tvalid_0's l2: 0.183755\tvalid_0's auc: 0.712439\n",
      "[3]\tvalid_0's l2: 0.18212\tvalid_0's auc: 0.716175\n",
      "[4]\tvalid_0's l2: 0.180617\tvalid_0's auc: 0.719581\n",
      "[5]\tvalid_0's l2: 0.179246\tvalid_0's auc: 0.723186\n",
      "[6]\tvalid_0's l2: 0.177917\tvalid_0's auc: 0.724594\n",
      "[7]\tvalid_0's l2: 0.176705\tvalid_0's auc: 0.726352\n",
      "[8]\tvalid_0's l2: 0.175644\tvalid_0's auc: 0.727929\n",
      "[9]\tvalid_0's l2: 0.174641\tvalid_0's auc: 0.728366\n",
      "[10]\tvalid_0's l2: 0.173646\tvalid_0's auc: 0.729264\n",
      "[11]\tvalid_0's l2: 0.172776\tvalid_0's auc: 0.730176\n",
      "[12]\tvalid_0's l2: 0.17199\tvalid_0's auc: 0.731295\n",
      "[13]\tvalid_0's l2: 0.17125\tvalid_0's auc: 0.732242\n",
      "[14]\tvalid_0's l2: 0.170516\tvalid_0's auc: 0.733075\n",
      "[15]\tvalid_0's l2: 0.169859\tvalid_0's auc: 0.733591\n",
      "[16]\tvalid_0's l2: 0.169215\tvalid_0's auc: 0.7345\n",
      "[17]\tvalid_0's l2: 0.168632\tvalid_0's auc: 0.735134\n",
      "[18]\tvalid_0's l2: 0.168061\tvalid_0's auc: 0.735961\n",
      "[19]\tvalid_0's l2: 0.167535\tvalid_0's auc: 0.736494\n",
      "[20]\tvalid_0's l2: 0.167033\tvalid_0's auc: 0.737089\n",
      "[21]\tvalid_0's l2: 0.166558\tvalid_0's auc: 0.737957\n",
      "[22]\tvalid_0's l2: 0.166081\tvalid_0's auc: 0.738949\n",
      "[23]\tvalid_0's l2: 0.16569\tvalid_0's auc: 0.739447\n",
      "[24]\tvalid_0's l2: 0.165285\tvalid_0's auc: 0.740055\n",
      "[25]\tvalid_0's l2: 0.164913\tvalid_0's auc: 0.740544\n",
      "[26]\tvalid_0's l2: 0.164582\tvalid_0's auc: 0.740959\n",
      "[27]\tvalid_0's l2: 0.164209\tvalid_0's auc: 0.741484\n",
      "[28]\tvalid_0's l2: 0.163861\tvalid_0's auc: 0.742068\n",
      "[29]\tvalid_0's l2: 0.163572\tvalid_0's auc: 0.74246\n",
      "[30]\tvalid_0's l2: 0.163234\tvalid_0's auc: 0.743274\n",
      "[31]\tvalid_0's l2: 0.162957\tvalid_0's auc: 0.743792\n",
      "[32]\tvalid_0's l2: 0.162679\tvalid_0's auc: 0.744295\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[32]\tvalid_0's l2: 0.162679\tvalid_0's auc: 0.744295\n",
      "Save model...\n",
      "Start predicting...\n",
      "The rmse of prediction is: 0.40333539428641396\n"
     ]
    }
   ],
   "source": [
    "gbm,y_pred,X_train ,y_train = lgb_pred('./data/train_lgb.txt', './data/valid_lgb.txt', '\\t', 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "查看每个特征的重要程度¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19,   0,  28,   6,  15,  62,  19,   8,  11,   0,  33,   0,  27,\n",
       "         0,  34,   0,  68,   0,   3,  22,   0,   3,  12,  36,   5, 162,\n",
       "        17, 149,   9,  13,  83,   0,   2,   8,   0,  20,  35,   0,  19])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17588.60017395,     0.        , 13697.72706604,  1527.25202942,\n",
       "        5273.76097107, 61430.39205933, 58175.77937317,  4058.06893921,\n",
       "        5047.81399536,     0.        , 60615.7379303 ,     0.        ,\n",
       "       15147.96902466,     0.        , 10214.86604309,     0.        ,\n",
       "       20004.60100555,     0.        ,   489.92799377,  4250.53800964,\n",
       "           0.        ,  1140.58499146,  1641.02298737,  5589.14899445,\n",
       "        1097.49501801, 46073.73500824,  6028.73600769, 40460.85097504,\n",
       "        2438.44700623,  7649.37095642, 23015.25582886,     0.        ,\n",
       "         379.0269928 ,  3998.13604736,     0.        ,  5586.1529541 ,\n",
       "       11735.71200562,     0.        ,  4622.90000153])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.feature_importance(\"gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_feat_impt(gbm):\n",
    "    gain = gbm.feature_importance(\"gain\").reshape(-1, 1) / sum(gbm.feature_importance(\"gain\"))\n",
    "    col = np.array(gbm.feature_name()).reshape(-1, 1)\n",
    "    return sorted(np.column_stack((col, gain)),key=lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['I6', '0.13993905549460622'], dtype='<U32'),\n",
       " array(['I11', '0.13808326513499225'], dtype='<U32'),\n",
       " array(['I7', '0.13252501482135512'], dtype='<U32'),\n",
       " array(['C13', '0.10495643514589945'], dtype='<U32'),\n",
       " array(['C15', '0.0921702284511962'], dtype='<U32'),\n",
       " array(['C18', '0.05242898595774266'], dtype='<U32'),\n",
       " array(['C4', '0.045570683767737805'], dtype='<U32'),\n",
       " array(['I1', '0.040067009395574664'], dtype='<U32'),\n",
       " array(['I13', '0.034507226910175645'], dtype='<U32'),\n",
       " array(['I3', '0.031203561035282972'], dtype='<U32'),\n",
       " array(['C24', '0.02673407084943356'], dtype='<U32'),\n",
       " array(['C2', '0.023269568338316327'], dtype='<U32'),\n",
       " array(['C17', '0.017425344538508466'], dtype='<U32'),\n",
       " array(['C14', '0.01373352170579938'], dtype='<U32'),\n",
       " array(['C11', '0.012732138035941956'], dtype='<U32'),\n",
       " array(['C23', '0.012725313025683329'], dtype='<U32'),\n",
       " array(['I5', '0.012013680923329197'], dtype='<U32'),\n",
       " array(['I9', '0.011498971423479259'], dtype='<U32'),\n",
       " array(['C26', '0.010531013040495989'], dtype='<U32'),\n",
       " array(['C7', '0.00968276865039378'], dtype='<U32'),\n",
       " array(['I8', '0.009244322157938918'], dtype='<U32'),\n",
       " array(['C21', '0.009107794423102904'], dtype='<U32'),\n",
       " array(['C16', '0.005554806985365049'], dtype='<U32'),\n",
       " array(['C10', '0.0037382669912820886'], dtype='<U32'),\n",
       " array(['I4', '0.003479095596406195'], dtype='<U32'),\n",
       " array(['C9', '0.0025982641663983208'], dtype='<U32'),\n",
       " array(['C12', '0.002500104770312809'], dtype='<U32'),\n",
       " array(['C6', '0.0011160609335351009'], dtype='<U32'),\n",
       " array(['C20', '0.0008634273297143932'], dtype='<U32'),\n",
       " array(['I2', '0.0'], dtype='<U32'),\n",
       " array(['I10', '0.0'], dtype='<U32'),\n",
       " array(['I12', '0.0'], dtype='<U32'),\n",
       " array(['C1', '0.0'], dtype='<U32'),\n",
       " array(['C3', '0.0'], dtype='<U32'),\n",
       " array(['C5', '0.0'], dtype='<U32'),\n",
       " array(['C8', '0.0'], dtype='<U32'),\n",
       " array(['C19', '0.0'], dtype='<U32'),\n",
       " array(['C22', '0.0'], dtype='<U32'),\n",
       " array(['C25', '0.0'], dtype='<U32')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_feat_impt(gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存GBDT参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = gbm.dump_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params_with_name((gbm, dump), 'gbm_dump') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm, dump = load_params_with_name('gbm_dump') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "通过eli5分析参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5 \n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train_eli5.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-64fb1453b5dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/train_eli5.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train_eli5.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open('./data/train_eli5.csv', 'rt') as f:\n",
    "    data = list(csv.DictReader(f))\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_all_xs = [{k: v for k, v in row.items() if k != 'clicked'} for row in data]\n",
    "_all_ys = np.array([int(row['clicked']) for row in data])\n",
    "\n",
    "all_xs, all_ys = shuffle(_all_xs, _all_ys, random_state=0)\n",
    "train_xs, valid_xs, train_ys, valid_ys = train_test_split(\n",
    "    all_xs, all_ys, test_size=0.25, random_state=0)\n",
    "print('{} items total, {:.1%} true'.format(len(all_xs), np.mean(all_ys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用LightGBM的输出生成FM数据\n",
    "GBDT已经训练好了，我们需要GBDT输出的叶子节点作为输入数据X传给FM，一共30个叶子节点，那么输入给FM的数据格式就是X中不是0的数据的index:value。\n",
    "\n",
    "一段真实数据如下：0 0:31 1:61 2:93 3:108 4:149 5:182 6:212 7:242 8:277 9:310 10:334 11:365 12:401 13:434 14:465 15:491 16:527 17:552 18:589 19:619 20:648 21:678 22:697 23:744 24:770 25:806 26:826 27:862 28:899 29:928 30:955 31:988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generat_lgb2fm_data(outdir, gbm, dump, tr_path, va_path, te_path, _sep = '\\t'):\n",
    "    with open(os.path.join(outdir, 'train_lgb2fm.txt'), 'w') as out_train:\n",
    "        with open(os.path.join(outdir, 'valid_lgb2fm.txt'), 'w') as out_valid:\n",
    "            with open(os.path.join(outdir, 'test_lgb2fm.txt'), 'w') as out_test:\n",
    "                df_train_ = pd.read_csv(tr_path, header=None, sep=_sep)\n",
    "                df_valid_ = pd.read_csv(va_path, header=None, sep=_sep)\n",
    "                df_test_= pd.read_csv(te_path, header=None, sep=_sep)\n",
    "\n",
    "                y_train_ = df_train_[0].values\n",
    "                y_valid_ = df_valid_[0].values                \n",
    "\n",
    "                X_train_ = df_train_.drop(0, axis=1).values\n",
    "                X_valid_ = df_valid_.drop(0, axis=1).values\n",
    "                X_test_= df_test_.values\n",
    "   \n",
    "                train_leaves= gbm.predict(X_train_, num_iteration=gbm.best_iteration, pred_leaf=True)\n",
    "                valid_leaves= gbm.predict(X_valid_, num_iteration=gbm.best_iteration, pred_leaf=True)\n",
    "                test_leaves= gbm.predict(X_test_, num_iteration=gbm.best_iteration, pred_leaf=True)\n",
    "\n",
    "                tree_info = dump['tree_info']\n",
    "                tree_counts = len(tree_info)\n",
    "                for i in range(tree_counts):\n",
    "                    train_leaves[:, i] = train_leaves[:, i] + tree_info[i]['num_leaves'] * i + 1\n",
    "                    valid_leaves[:, i] = valid_leaves[:, i] + tree_info[i]['num_leaves'] * i + 1\n",
    "                    test_leaves[:, i] = test_leaves[:, i] + tree_info[i]['num_leaves'] * i + 1\n",
    "#                     print(train_leaves[:, i])\n",
    "#                     print(tree_info[i]['num_leaves'])\n",
    "\n",
    "                for idx in range(len(y_train_)):            \n",
    "                    out_train.write((str(y_train_[idx]) + '\\t'))\n",
    "                    out_train.write('\\t'.join(\n",
    "                        ['{}:{}'.format(ii, val) for ii,val in enumerate(train_leaves[idx]) if float(val) != 0 ]) + '\\n')\n",
    "                    \n",
    "                for idx in range(len(y_valid_)):                   \n",
    "                    out_valid.write((str(y_valid_[idx]) + '\\t'))\n",
    "                    out_valid.write('\\t'.join(\n",
    "                        ['{}:{}'.format(ii, val) for ii,val in enumerate(valid_leaves[idx]) if float(val) != 0 ]) + '\\n')\n",
    "                    \n",
    "                for idx in range(len(X_test_)):                   \n",
    "                    out_test.write('\\t'.join(\n",
    "                        ['{}:{}'.format(ii, val) for ii,val in enumerate(test_leaves[idx]) if float(val) != 0 ]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generat_lgb2fm_data('./data', gbm, dump, './data/train_lgb.txt', './data/valid_lgb.txt', './data/test_lgb.txt', '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练FM\n",
    "为训练FM的数据已经准备好了，我们调用LibFM进行训练。\n",
    "\n",
    "迭代64次，使用sgd训练，学习率是0.00000001，训练好的模型保存为文件fm_model。\n",
    "\n",
    "训练输出的log，Train和Test的数值不是loss，是accuracy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libfm/libfm/bin/libFM -task c -train ./data/train_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 64 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -save_model fm_model'\n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM模型训练好了，我们把训练、验证和测试数据输入给FM，得到FM层的输出，输出的文件名为*.fm.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libfm/libfm/bin/libFM -task c -train ./data/train_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 32 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -load_model fm_model -train_off true -prefix tr'\n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libfm/libfm/bin/libFM -task c -train ./data/valid_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 32 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -load_model fm_model -train_off true -prefix va'\n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = './libfm/libfm/bin/libFM -task c -train ./data/test_lgb2fm.txt -test ./data/valid_lgb2fm.txt -dim ’1,1,8’ -iter 32 -method sgd -learn_rate 0.00000001 -regular ’0,0,0.01’ -init_stdev 0.1 -load_model fm_model -train_off true -prefix te -test2predict true'\n",
    "os.popen(cmd).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "sparse_max = 30000 # sparse_feature_dim = 117568\n",
    "sparse_dim = 26\n",
    "dense_dim = 13\n",
    "out_dim = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def get_inputs():\n",
    "    dense_input = tf.placeholder(tf.float32, [None, dense_dim], name=\"dense_input\")\n",
    "    sparse_input = tf.placeholder(tf.int32, [None, sparse_dim], name=\"sparse_input\")\n",
    "    FFM_input = tf.placeholder(tf.float32, [None, 1], name=\"FFM_input\")\n",
    "    FM_input = tf.placeholder(tf.float32, [None, 1], name=\"FM_input\")\n",
    "    \n",
    "    targets = tf.placeholder(tf.float32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    return dense_input, sparse_input, FFM_input, FM_input, targets, LearningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_embedding(sparse_input):\n",
    "    with tf.name_scope(\"sparse_embedding\"):\n",
    "        sparse_embed_matrix = tf.Variable(tf.random_uniform([sparse_max, embed_dim], -1, 1), name = \"sparse_embed_matrix\")\n",
    "        sparse_embed_layer = tf.nn.embedding_lookup(sparse_embed_matrix, sparse_input, name = \"sparse_embed_layer\")\n",
    "        sparse_embed_layer = tf.reshape(sparse_embed_layer, [-1, sparse_dim * embed_dim])\n",
    "    return sparse_embed_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dnn_layer(dense_input, sparse_embed_layer):\n",
    "    with tf.name_scope(\"dnn_layer\"):\n",
    "        input_combine_layer = tf.concat([dense_input, sparse_embed_layer], 1)  #(?, 845 = 832 + 13)\n",
    "        fc1_layer = tf.layers.dense(input_combine_layer, out_dim, name = \"fc1_layer\", activation=tf.nn.relu)\n",
    "        fc2_layer = tf.layers.dense(fc1_layer, out_dim, name = \"fc2_layer\", activation=tf.nn.relu)\n",
    "        fc3_layer = tf.layers.dense(fc2_layer, out_dim, name = \"fc3_layer\", activation=tf.nn.relu)\n",
    "    return fc3_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    dense_input, sparse_input, FFM_input, FM_input, targets, lr = get_inputs()\n",
    "    sparse_embed_layer = get_sparse_embedding(sparse_input)\n",
    "    fc3_layer = get_dnn_layer(dense_input, sparse_embed_layer)\n",
    "\n",
    "    ffm_fc_layer = tf.layers.dense(FFM_input, 1, name = \"ffm_fc_layer\")\n",
    "    fm_fc_layer = tf.layers.dense(FM_input, 1, name = \"fm_fc_layer\")\n",
    "    feature_combine_layer = tf.concat([ffm_fc_layer, fm_fc_layer, fc3_layer], 1)  #(?, 402)\n",
    "\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        logits = tf.layers.dense(feature_combine_layer, 1, name = \"logits_layer\")\n",
    "        pred = tf.nn.sigmoid(logits, name = \"prediction\")\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # LogLoss损失，Logistic回归到点击率\n",
    "#         cost = tf.losses.sigmoid_cross_entropy(targets, logits )\n",
    "        sigmoid_cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits, name = \"sigmoid_cost\")\n",
    "        logloss_cost = tf.losses.log_loss(labels=targets, predictions=pred)\n",
    "        cost = logloss_cost # + sigmoid_cost\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.FtrlOptimizer(lr)  #tf.train.FtrlOptimizer(lr)  AdamOptimizer\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "    \n",
    "    # Accuracy\n",
    "    with tf.name_scope(\"score\"):\n",
    "        correct_prediction = tf.equal(tf.to_float(pred > 0.5), targets)\n",
    "        accuracy = tf.reduce_mean(tf.to_float(correct_prediction), name=\"accuracy\")\n",
    "        \n",
    "#     auc, uop = tf.contrib.metrics.streaming_auc(pred, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 1\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 25\n",
    "\n",
    "save_dir = './save'\n",
    "\n",
    "ffm_tr_out_path = './tr_ffm.out.logit'\n",
    "ffm_va_out_path = './va_ffm.out.logit'\n",
    "fm_tr_out_path = './tr.fm.logits'\n",
    "fm_va_out_path = './va.fm.logits'\n",
    "train_path = './data/train.txt'\n",
    "valid_path = './data/valid.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches_downsample(Xs, ys, batch_size):\n",
    "    ind_0 = ys==0\n",
    "    ind_1 = ys==1\n",
    "    Xs_0 = Xs[ind_0]\n",
    "    ys_0 = ys[ind_0]\n",
    "    Xs_1 = Xs[ind_1]\n",
    "    ys_1 = ys[ind_1]\n",
    "    sampling_ind = np.random.permutation(Xs_0.shape[0])[:Xs_1.shape[0]]\n",
    "    Xs_0_sampling = Xs_0[sampling_ind]\n",
    "    ys_0_sampling = ys_0[sampling_ind]\n",
    "    Xs_downsampled = np.concatenate((Xs_0_sampling, Xs_1))\n",
    "    ys_downsampled = np.concatenate((ys_0_sampling, ys_1))\n",
    "    downsampled_ind = np.random.permutation(Xs_downsampled.shape[0])\n",
    "    Xs_downsampled = Xs_downsampled[downsampled_ind]\n",
    "    ys_downsampled = ys_downsampled[downsampled_ind]\n",
    "    for start in range(0, len(Xs_downsampled), batch_size):\n",
    "        end = min(start + batch_size, len(Xs_downsampled))\n",
    "        yield Xs_downsampled[start:end], ys_downsampled[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./tr_ffm.out.logit' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ed5b7498655c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mffm_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mffm_tr_out_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mffm_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mffm_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mffm_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mffm_va_out_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mffm_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mffm_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python3.6\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python3.6\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python3.6\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python3.6\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\python3.6\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./tr_ffm.out.logit' does not exist"
     ]
    }
   ],
   "source": [
    "ffm_train = pd.read_csv(ffm_tr_out_path, header=None)    \n",
    "ffm_train = ffm_train[0].values\n",
    "\n",
    "ffm_valid = pd.read_csv(ffm_va_out_path, header=None)    \n",
    "ffm_valid = ffm_valid[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
